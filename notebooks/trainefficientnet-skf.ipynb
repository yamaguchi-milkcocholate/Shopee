{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from shutil import copyfile\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accredited-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    outdir: str = \"../results/efficientnet\"\n",
    "    device: str = \"cuda:1\"\n",
    "    device_id: int = 1\n",
    "        \n",
    "    tf_expt: int = -1 # tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    datadir: str = '../data/tfrecord-skf'\n",
    "#     modeldir: str = '../models/bert/bert_en_uncased_L-24_H-1024_A-16_1'\n",
    "    seed: int = 123\n",
    "    valid_ratio: float = 0.25\n",
    "    image_size: List[int] = field(default_factory=lambda: [512, 512])\n",
    "    \n",
    "    # Training config\n",
    "    en_type: str = 'B0'\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 20\n",
    "    lr: float = 0.001\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "subsequent-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fallen-source",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path().resolve()\n",
    "sys.path.append(os.path.abspath(base_dir / '../'))\n",
    "\n",
    "config_dict = {\n",
    "#     'epochs': 1,\n",
    "}\n",
    "config = Config().update(config_dict)\n",
    "config.to_yaml(base_dir / config.outdir / 'config.yaml')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.device_id)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from src.tokenization import *\n",
    "from src.preprocess import *\n",
    "from src.image import *\n",
    "from src.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "established-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def data_augment(posting_id, image, label_group, matches):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.random_hue(image, 0.01)\n",
    "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
    "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
    "    image = tf.image.random_brightness(image, 0.10)\n",
    "    return posting_id, image, label_group, matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beautiful-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size):\n",
    "    lr_start   = 0.000001\n",
    "    lr_max     = 0.000005 * 256\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "renewable-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data_items(filenames):\n",
    "    # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "grand-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_files = sorted(tf.io.gfile.glob(str(base_dir / config.datadir) + '/*.tfrec'))\n",
    "# train_files, valid_files = tfrecord_files[:int(len(tfrecord_files) * (1 - config.valid_ratio))], tfrecord_files[int(len(tfrecord_files) * (1 - config.valid_ratio)):]\n",
    "train_files, valid_files = tfrecord_files[int(len(tfrecord_files) * config.valid_ratio):], tfrecord_files[:int(len(tfrecord_files) * config.valid_ratio)]\n",
    "steps_per_epoch = count_data_items(train_files) // config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intimate-louisville",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11014"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(base_dir / config.datadir / 'train_folds.csv')\n",
    "n_classes = train['label_group'].nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "other-grenada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/20\n",
      "  2/856 [..............................] - ETA: 6:36 - loss: 24.0891 - sparse_categorical_accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2926s vs `on_train_batch_end` time: 0.6322s). Check your callbacks.\n",
      "856/856 [==============================] - ETA: 0s - loss: 23.9871 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 00001: val_loss improved from inf to 24.06334, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_01.h5\n",
      "856/856 [==============================] - 675s 789ms/step - loss: 23.9871 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 24.0633 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00025680000000000006.\n",
      "Epoch 2/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 19.5395 - sparse_categorical_accuracy: 0.0078\n",
      "Epoch 00002: val_loss improved from 24.06334 to 18.93654, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_02.h5\n",
      "856/856 [==============================] - 655s 765ms/step - loss: 19.5395 - sparse_categorical_accuracy: 0.0078 - val_loss: 18.9365 - val_sparse_categorical_accuracy: 0.0414\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0005126000000000001.\n",
      "Epoch 3/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 12.7926 - sparse_categorical_accuracy: 0.0946\n",
      "Epoch 00003: val_loss improved from 18.93654 to 16.29680, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_03.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 12.7926 - sparse_categorical_accuracy: 0.0946 - val_loss: 16.2968 - val_sparse_categorical_accuracy: 0.1283\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0007684000000000001.\n",
      "Epoch 4/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 8.4000 - sparse_categorical_accuracy: 0.2223\n",
      "Epoch 00004: val_loss improved from 16.29680 to 15.36747, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_04.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 8.4000 - sparse_categorical_accuracy: 0.2223 - val_loss: 15.3675 - val_sparse_categorical_accuracy: 0.1767\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "Epoch 5/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 6.3483 - sparse_categorical_accuracy: 0.3270\n",
      "Epoch 00005: val_loss improved from 15.36747 to 15.05334, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_05.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 6.3483 - sparse_categorical_accuracy: 0.3270 - val_loss: 15.0533 - val_sparse_categorical_accuracy: 0.1988\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00128.\n",
      "Epoch 6/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 5.3973 - sparse_categorical_accuracy: 0.3913\n",
      "Epoch 00006: val_loss improved from 15.05334 to 14.95898, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_06.h5\n",
      "856/856 [==============================] - 657s 768ms/step - loss: 5.3973 - sparse_categorical_accuracy: 0.3913 - val_loss: 14.9590 - val_sparse_categorical_accuracy: 0.2192\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0010242.\n",
      "Epoch 7/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 3.6950 - sparse_categorical_accuracy: 0.5455\n",
      "Epoch 00007: val_loss improved from 14.95898 to 14.51448, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_07.h5\n",
      "856/856 [==============================] - 652s 762ms/step - loss: 3.6950 - sparse_categorical_accuracy: 0.5455 - val_loss: 14.5145 - val_sparse_categorical_accuracy: 0.2809\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0008195600000000003.\n",
      "Epoch 8/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 2.5338 - sparse_categorical_accuracy: 0.6740\n",
      "Epoch 00008: val_loss improved from 14.51448 to 14.34340, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_08.h5\n",
      "856/856 [==============================] - 654s 763ms/step - loss: 2.5338 - sparse_categorical_accuracy: 0.6740 - val_loss: 14.3434 - val_sparse_categorical_accuracy: 0.3002\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0006558480000000003.\n",
      "Epoch 9/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 1.8360 - sparse_categorical_accuracy: 0.7632\n",
      "Epoch 00009: val_loss improved from 14.34340 to 14.27528, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_09.h5\n",
      "856/856 [==============================] - 656s 766ms/step - loss: 1.8360 - sparse_categorical_accuracy: 0.7632 - val_loss: 14.2753 - val_sparse_categorical_accuracy: 0.3149\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0005248784000000002.\n",
      "Epoch 10/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 1.3536 - sparse_categorical_accuracy: 0.8308\n",
      "Epoch 00010: val_loss did not improve from 14.27528\n",
      "856/856 [==============================] - 652s 761ms/step - loss: 1.3536 - sparse_categorical_accuracy: 0.8308 - val_loss: 14.2930 - val_sparse_categorical_accuracy: 0.3263\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0004201027200000002.\n",
      "Epoch 11/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 1.0290 - sparse_categorical_accuracy: 0.8797\n",
      "Epoch 00011: val_loss improved from 14.27528 to 14.18855, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_11.h5\n",
      "856/856 [==============================] - 653s 763ms/step - loss: 1.0290 - sparse_categorical_accuracy: 0.8797 - val_loss: 14.1885 - val_sparse_categorical_accuracy: 0.3323\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0003362821760000002.\n",
      "Epoch 12/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.7890 - sparse_categorical_accuracy: 0.9170\n",
      "Epoch 00012: val_loss did not improve from 14.18855\n",
      "856/856 [==============================] - 651s 760ms/step - loss: 0.7890 - sparse_categorical_accuracy: 0.9170 - val_loss: 14.2143 - val_sparse_categorical_accuracy: 0.3367\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.00026922574080000017.\n",
      "Epoch 13/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.6253 - sparse_categorical_accuracy: 0.9387\n",
      "Epoch 00013: val_loss improved from 14.18855 to 14.15554, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_13.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 0.6253 - sparse_categorical_accuracy: 0.9387 - val_loss: 14.1555 - val_sparse_categorical_accuracy: 0.3386\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.00021558059264000014.\n",
      "Epoch 14/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.5210 - sparse_categorical_accuracy: 0.9530\n",
      "Epoch 00014: val_loss improved from 14.15554 to 14.10463, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_14.h5\n",
      "856/856 [==============================] - 655s 765ms/step - loss: 0.5210 - sparse_categorical_accuracy: 0.9530 - val_loss: 14.1046 - val_sparse_categorical_accuracy: 0.3437\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0001726644741120001.\n",
      "Epoch 15/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.4327 - sparse_categorical_accuracy: 0.9633\n",
      "Epoch 00015: val_loss improved from 14.10463 to 14.09639, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_15.h5\n",
      "856/856 [==============================] - 652s 762ms/step - loss: 0.4327 - sparse_categorical_accuracy: 0.9633 - val_loss: 14.0964 - val_sparse_categorical_accuracy: 0.3481\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0001383315792896001.\n",
      "Epoch 16/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.3772 - sparse_categorical_accuracy: 0.9705\n",
      "Epoch 00016: val_loss improved from 14.09639 to 14.07307, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_16.h5\n",
      "856/856 [==============================] - 653s 763ms/step - loss: 0.3772 - sparse_categorical_accuracy: 0.9705 - val_loss: 14.0731 - val_sparse_categorical_accuracy: 0.3498\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00011086526343168008.\n",
      "Epoch 17/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.3408 - sparse_categorical_accuracy: 0.9739\n",
      "Epoch 00017: val_loss did not improve from 14.07307\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 0.3408 - sparse_categorical_accuracy: 0.9739 - val_loss: 14.0796 - val_sparse_categorical_accuracy: 0.3501\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 8.889221074534406e-05.\n",
      "Epoch 18/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.3065 - sparse_categorical_accuracy: 0.9774\n",
      "Epoch 00018: val_loss did not improve from 14.07307\n",
      "856/856 [==============================] - 655s 765ms/step - loss: 0.3065 - sparse_categorical_accuracy: 0.9774 - val_loss: 14.0736 - val_sparse_categorical_accuracy: 0.3503\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 7.131376859627525e-05.\n",
      "Epoch 19/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.2825 - sparse_categorical_accuracy: 0.9788\n",
      "Epoch 00019: val_loss improved from 14.07307 to 14.06741, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_19.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 0.2825 - sparse_categorical_accuracy: 0.9788 - val_loss: 14.0674 - val_sparse_categorical_accuracy: 0.3517\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 5.725101487702021e-05.\n",
      "Epoch 20/20\n",
      "856/856 [==============================] - ETA: 0s - loss: 0.2601 - sparse_categorical_accuracy: 0.9806\n",
      "Epoch 00020: val_loss improved from 14.06741 to 14.06554, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/efficientnet/B0/EfficientNetB0_123_20.h5\n",
      "856/856 [==============================] - 654s 764ms/step - loss: 0.2601 - sparse_categorical_accuracy: 0.9806 - val_loss: 14.0655 - val_sparse_categorical_accuracy: 0.3514\n"
     ]
    }
   ],
   "source": [
    "seed_everything(config.seed)\n",
    "\n",
    "outdir = base_dir / config.outdir / config.en_type\n",
    "\n",
    "train_dataset = get_training_dataset(train_files, config, data_augment)\n",
    "train_dataset = train_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "\n",
    "valid_dataset = get_validation_dataset(valid_files, config, data_augment)\n",
    "valid_dataset = valid_dataset.map(lambda posting_id, image, label_group, matches: (image, label_group))\n",
    "\n",
    "model = build_efficientnet_model(\n",
    "    n_classes=n_classes,\n",
    "    image_size=config.image_size,\n",
    "    lr=config.lr,\n",
    "    en_type=config.en_type,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "# Model checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    str(outdir / (f'EfficientNet{config.en_type}_{config.seed}_' + '{epoch:02d}.h5')),\n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    save_weights_only = True, \n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs = config.epochs,\n",
    "    callbacks = [checkpoint, get_lr_callback(config.batch_size)], \n",
    "    validation_data = valid_dataset,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "pickle.dump(history.history, open(str(outdir / 'history.pkl'), 'wb'))\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-association",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
