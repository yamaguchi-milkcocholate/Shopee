{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handmade-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from shutil import copyfile\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smaller-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    outdir: str = \"../results/efficientnet-tpu\"\n",
    "    device: str = \"cuda:2\"\n",
    "    device_id: int = 2\n",
    "\n",
    "    datadir: str = '../data/tfrecord-gkf'\n",
    "    image_dir: str = '../data/shopee-product-matching/train_images'\n",
    "    seed: int = 42\n",
    "    n_splits: int = 1\n",
    "    tf_expt: int = -1\n",
    "    image_size: List[int] = field(default_factory=lambda: [512, 512])\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 20\n",
    "    epochs: int = 100\n",
    "    patience: int = 5\n",
    "    lr: float = 0.00001\n",
    "    emb_len: int = 2048\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wicked-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "crucial-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.device_id)\n",
    "\n",
    "base_dir = Path().resolve()\n",
    "sys.path.append(os.path.abspath(base_dir / '../'))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_addons as tfa\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from src.tokenization import *\n",
    "from src.preprocess import *\n",
    "from src.image import *\n",
    "from src.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understood-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get our f1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def precision_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    return intersection / len_y_pred\n",
    "\n",
    "\n",
    "def recall_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    return intersection / len_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liberal-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read our test image and return image\n",
    "def read_image(image, image_size):\n",
    "    image = tf.io.read_file(image)\n",
    "    image = decode_image(image, image_size)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Function to get our dataset that read images\n",
    "def get_dataset(image, config):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(image)\n",
    "    dataset = dataset.map(lambda x: read_image(x, config.image_size), num_parallel_calls = config.tf_expt)\n",
    "    dataset = dataset.batch(config.batch_size)\n",
    "    dataset = dataset.prefetch(config.tf_expt)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-census",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11014"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(base_dir / config.datadir / 'train_folds.csv')\n",
    "n_classes = train['label_group'].nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "broken-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(df: pd.DataFrame, weight_path: Path, en_type: str):\n",
    "    image_paths = [str(base_dir / config.image_dir / filename) for filename in df['image']]\n",
    "    \n",
    "    model = build_efficientnet_model(\n",
    "        n_classes=n_classes,\n",
    "        image_size=config.image_size,\n",
    "        lr=config.lr,\n",
    "        en_type=en_type,\n",
    "        train=False,\n",
    "        emb_len=config.emb_len\n",
    "    )\n",
    "    model.load_weights(str(weight_path))\n",
    "    model = tf.keras.models.Model(inputs=model.input[0:3], outputs=model.layers[-4].output)\n",
    "    \n",
    "    chunk = 500\n",
    "    iterator = np.arange(np.ceil(len(df) / chunk))\n",
    "\n",
    "    embeds = []\n",
    "    for j in tqdm(iterator):\n",
    "        a = int(j * chunk)\n",
    "        b = int((j + 1) * chunk)\n",
    "\n",
    "        image_dataset = get_dataset(image_paths[a:b], config)\n",
    "        embeddings = model.predict(image_dataset)\n",
    "        \n",
    "        embeds.append(embeddings)\n",
    "    \n",
    "    del model\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "    return np.concatenate(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "silver-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_df(fold: int):\n",
    "    valid_folds = [i for i in range(train.fold.unique().shape[0]) if (i % config.n_splits) == fold]\n",
    "    \n",
    "    valid_df = train.query(f'fold in {valid_folds}').copy()\n",
    "    return valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "weird-portuguese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_list(epoch: int, en_type: str):\n",
    "    epoch = format(epoch, '02')\n",
    "    embeddings_list = list()\n",
    "    for i in range(config.n_splits):\n",
    "        outdir = base_dir / config.outdir / f'EfficientNet{en_type}_GKF_seed{config.seed}_fold{i}_emb{config.emb_len}'\n",
    "        weight_path = outdir / f'epoch{epoch}.h5'\n",
    "        emb_outpath = outdir / f'embeddings_epoch{epoch}.pkl'\n",
    "\n",
    "        if os.path.exists(emb_outpath):\n",
    "            print('get embeddings from the cache')\n",
    "            embeddings = pickle.load(open(str(emb_outpath), 'rb'))\n",
    "        else:\n",
    "            embeddings, cosines = get_embeddings(\n",
    "                df=train,\n",
    "                weight_path=weight_path,\n",
    "                en_type=en_type\n",
    "            )\n",
    "            pickle.dump(\n",
    "                embeddings,\n",
    "                open(str(emb_outpath), 'wb')\n",
    "            )\n",
    "        embeddings_list += [embeddings]\n",
    "                \n",
    "    return embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "unknown-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(df: pd.DataFrame, distance: np.ndarray, indice: np.ndarray, thr: float) -> List[str]:\n",
    "        idx = np.where(distance < thr)[0]\n",
    "        ids = indice[idx]\n",
    "        return df['posting_id'].iloc[ids].values.tolist()\n",
    "    \n",
    "    \n",
    "def search_thresholds(df: pd.DataFrame, embeddings: np.ndarray, fold: int, knn: int = 50, metric='euclidean',\n",
    "                      thr_start: int = 3, thr_end: int = 5, thr_interval: float = 0.1, verbose=False):\n",
    "    # KNN\n",
    "    knn_model = NearestNeighbors(n_neighbors=knn, metric=metric)\n",
    "    knn_model.fit(embeddings)\n",
    "\n",
    "    valid_df = get_valid_df(fold=fold)\n",
    "    valid_embeddings = embeddings[valid_df.index, :]\n",
    "    distances, indices = knn_model.kneighbors(valid_embeddings)\n",
    "\n",
    "    # grid search\n",
    "    thresholds = list(np.arange(thr_start, thr_end, thr_interval))\n",
    "    scores = []\n",
    "    for threshold in thresholds:\n",
    "        predictions = []\n",
    "        for k in range(valid_embeddings.shape[0]):\n",
    "            matches = get_matches(df=df, distance=distances[k, ], indice=indices[k, ], thr=threshold)\n",
    "            posting_ids = ' '.join(matches)\n",
    "            predictions.append(posting_ids)\n",
    "        valid_df['pred_matches'] = predictions\n",
    "        valid_df['f1'] = f1_score(valid_df['matches'], valid_df['pred_matches'])\n",
    "        score = valid_df['f1'].mean()\n",
    "        if verbose:\n",
    "            print(f'Our f1 score for threshold {np.round(threshold, 2)} is {score}')\n",
    "        scores.append(score)\n",
    "    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "\n",
    "    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "    best_threshold = max_score['thresholds'].values[0]\n",
    "    best_score = max_score['scores'].values[0]\n",
    "    print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "    \n",
    "    return best_score, best_threshold\n",
    "\n",
    "\n",
    "def database_augment(embeddings, n_aug):\n",
    "    weights = np.logspace(0, -1.5, n_aug)\n",
    "    model = NearestNeighbors(n_neighbors=n_aug, metric='cosine')\n",
    "    model.fit(embeddings)\n",
    "    \n",
    "    dba_embeddings = list()\n",
    "    \n",
    "    CHUNK = 1024*4\n",
    "    CTS = len(embeddings) // CHUNK\n",
    "    if len(embeddings) % CHUNK != 0: CTS += 1\n",
    "    for j in tqdm(range( CTS )):\n",
    "\n",
    "        a, b = j * CHUNK, (j + 1) * CHUNK\n",
    "        b = min(b, len(embeddings))\n",
    "\n",
    "        # COSINE SIMILARITY DISTANCE\n",
    "        distances, indices = model.kneighbors(embeddings[a:b, :])\n",
    "    \n",
    "        for k in range(b - a):\n",
    "            neighbor_embeddings = embeddings[indices[k, ]]\n",
    "            comb = np.dot(weights, neighbor_embeddings)\n",
    "            dba_embeddings.append(comb)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return np.vstack(dba_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indonesian-transcript",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get embeddings from the cache\n",
      "get embeddings from the cache\n",
      "get embeddings from the cache\n"
     ]
    }
   ],
   "source": [
    "embs = list()\n",
    "for entype in ['B0', 'B3', 'B5']:\n",
    "    embs_bx = get_embeddings_list(epoch=20, en_type=entype)[0]\n",
    "    embs.append(embs_bx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "transparent-southeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.7481825997918536 and has a threshold 0.39999999999999997\n",
      "Our best score is 0.7507596436161528 and has a threshold 0.35\n",
      "Our best score is 0.7488550273098631 and has a threshold 0.35\n"
     ]
    }
   ],
   "source": [
    "for emb in embs:\n",
    "    search_thresholds(train, emb, 0, 50, 'cosine', 0.2, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "alpine-torture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Our best score is 0.7535258462780937 and has a threshold 0.35\n",
      "Our best score is 0.7542562888347566 and has a threshold 0.35\n",
      "Our best score is 0.7528132094289697 and has a threshold 0.35\n",
      "3\n",
      "Our best score is 0.7674997507953082 and has a threshold 0.3\n",
      "Our best score is 0.7663347251207753 and has a threshold 0.3\n",
      "Our best score is 0.7655683224935339 and has a threshold 0.3\n",
      "4\n",
      "Our best score is 0.7695514976063892 and has a threshold 0.2\n",
      "Our best score is 0.7701235593847349 and has a threshold 0.2\n",
      "Our best score is 0.770026601127599 and has a threshold 0.2\n",
      "5\n",
      "Our best score is 0.7659990107680901 and has a threshold 0.2\n",
      "Our best score is 0.7642804654293885 and has a threshold 0.2\n",
      "Our best score is 0.7637633025248763 and has a threshold 0.2\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for n_aug in range(2, 6):\n",
    "    print(n_aug)\n",
    "    dba_embs = [database_augment(emb, n_aug=n_aug) for emb in embs]\n",
    "    results += [search_thresholds(train, emb, 0, 50, 'cosine', 0.2, 0.5, 0.05) for emb in dba_embs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "transparent-lyric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.7710984633445934 and has a threshold 0.21999999999999995\n",
      "Our best score is 0.7701235593847349 and has a threshold 0.19999999999999996\n",
      "Our best score is 0.7700500150471253 and has a threshold 0.20999999999999996\n"
     ]
    }
   ],
   "source": [
    "dba_embs = [database_augment(emb, n_aug=4) for emb in embs]\n",
    "for emb in dba_embs:\n",
    "    search_thresholds(train, emb, 0, 50, 'cosine', 0.1, 0.3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "julian-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(embeddings, valid_idx, thr):\n",
    "    valid_embeddings = embeddings[valid_idx, :]\n",
    "    # KNN\n",
    "    knn_model = NearestNeighbors(n_neighbors=50, metric='cosine')\n",
    "    knn_model.fit(embeddings)\n",
    "    distances, indices = knn_model.kneighbors(valid_embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in range(valid_embeddings.shape[0]):\n",
    "        matches = get_matches(df=train, distance=distances[k, ], indice=indices[k, ], thr=thr)\n",
    "        predictions.append(matches)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "reserved-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = get_valid_df(fold=0)\n",
    "\n",
    "for thr, emb, entype in zip([0.22, 0.2, 0.21], dba_embs, ['B0', 'B3', 'B5']):\n",
    "    valid_df[f'preds_{entype}'] = get_prediction(embeddings=emb, valid_idx=valid_df.index, thr=thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "limited-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or:  0.7686572817155032\n",
      "and:  0.7686190196705233\n"
     ]
    }
   ],
   "source": [
    "def concat_matches_or(row):\n",
    "    matches = np.concatenate([row['preds_B0'], row['preds_B3'], row['preds_B5']])\n",
    "    return ' '.join( np.unique(matches) )\n",
    "\n",
    "\n",
    "def concat_matches_and(row):\n",
    "    matches = list(set(row['preds_B0']) & set(row['preds_B3']) & set(row['preds_B5']))\n",
    "    return ' '.join(matches)\n",
    "\n",
    "print('or: ', f1_score(valid_df['matches'], valid_df.apply(concat_matches_or, axis=1)).mean())\n",
    "print('and: ', f1_score(valid_df['matches'], valid_df.apply(concat_matches_and, axis=1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "second-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 0.0 is 0.19493150498865816\n",
      "Our f1 score for threshold 0.1 is 0.6502414180236382\n",
      "Our f1 score for threshold 0.2 is 0.7169371778230083\n",
      "Our f1 score for threshold 0.3 is 0.7467511391998424\n",
      "Our f1 score for threshold 0.4 is 0.7541854012158323\n",
      "Our f1 score for threshold 0.5 is 0.7207435421563243\n",
      "Our f1 score for threshold 0.6 is 0.5853674767476909\n",
      "Our f1 score for threshold 0.7 is 0.29580146366960736\n",
      "Our f1 score for threshold 0.8 is 0.16855946599379565\n",
      "Our f1 score for threshold 0.9 is 0.16831691175633612\n",
      "Our best score is 0.7541854012158323 and has a threshold 0.4\n",
      "Our f1 score for threshold 0.0 is 0.21253980169956915\n",
      "Our f1 score for threshold 0.1 is 0.6501933612599894\n",
      "Our f1 score for threshold 0.2 is 0.7174988238007374\n",
      "Our f1 score for threshold 0.3 is 0.7467027604485956\n",
      "Our f1 score for threshold 0.4 is 0.7540204219176603\n",
      "Our f1 score for threshold 0.5 is 0.718966418334804\n",
      "Our f1 score for threshold 0.6 is 0.5791827488991553\n",
      "Our f1 score for threshold 0.7 is 0.28524882500648036\n",
      "Our f1 score for threshold 0.8 is 0.16821132751039\n",
      "Our f1 score for threshold 0.9 is 0.16808686186185318\n",
      "Our best score is 0.7540204219176603 and has a threshold 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7540204219176603, 0.4)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_thresholds(train, np.hstack(embs), 0, 50, 'cosine', 0, 1, 0.1, verbose=True)\n",
    "search_thresholds(train, np.mean(embs, axis=0), 0, 50, 'cosine', 0, 1, 0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "headed-actress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Our best score is 0.7574578052770922 and has a threshold 0.4\n",
      "3\n",
      "Our best score is 0.7729341902137608 and has a threshold 0.30000000000000004\n",
      "4\n",
      "Our best score is 0.7757328548663609 and has a threshold 0.2\n",
      "5\n",
      "Our best score is 0.7754705742133092 and has a threshold 0.2\n"
     ]
    }
   ],
   "source": [
    "embs_ens = np.hstack(embs)\n",
    "results = list()\n",
    "for n_aug in range(2, 6):\n",
    "    print(n_aug)\n",
    "    dba_embs_ens = database_augment(embs_ens, n_aug=n_aug)\n",
    "    results += [search_thresholds(train, dba_embs_ens, 0, 50, 'cosine', 0, 1, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "going-switch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Our best score is 0.7577914755076941 and has a threshold 0.4\n",
      "3\n",
      "Our best score is 0.7732484741852182 and has a threshold 0.30000000000000004\n",
      "4\n",
      "Our best score is 0.7756342948285152 and has a threshold 0.2\n",
      "5\n",
      "Our best score is 0.7745672538166846 and has a threshold 0.2\n"
     ]
    }
   ],
   "source": [
    "embs_ens = np.mean(embs, axis=0)\n",
    "results = list()\n",
    "for n_aug in range(2, 6):\n",
    "    print(n_aug)\n",
    "    dba_embs_ens = database_augment(embs_ens, n_aug=n_aug)\n",
    "    results += [search_thresholds(train, dba_embs_ens, 0, 50, 'cosine', 0, 1, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eleven-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 0.0 is 0.19528915944629796\n",
      "Our f1 score for threshold 0.1 is 0.7321883175346043\n",
      "Our f1 score for threshold 0.2 is 0.7729401129883415\n",
      "Our f1 score for threshold 0.3 is 0.7730499733199564\n",
      "Our f1 score for threshold 0.4 is 0.7413945458839016\n",
      "Our f1 score for threshold 0.5 is 0.6581999194805409\n",
      "Our f1 score for threshold 0.6 is 0.48438001215985527\n",
      "Our f1 score for threshold 0.7 is 0.24341859390526474\n",
      "Our f1 score for threshold 0.8 is 0.16968138855616677\n",
      "Our f1 score for threshold 0.9 is 0.16958922760181622\n",
      "Our best score is 0.7730499733199564 and has a threshold 0.30000000000000004\n",
      "Our f1 score for threshold 0.0 is 0.212138935893935\n",
      "Our f1 score for threshold 0.1 is 0.732263754640866\n",
      "Our f1 score for threshold 0.2 is 0.7731480665958421\n",
      "Our f1 score for threshold 0.3 is 0.7729552706400833\n",
      "Our f1 score for threshold 0.4 is 0.7407169284205879\n",
      "Our f1 score for threshold 0.5 is 0.6566666768203135\n",
      "Our f1 score for threshold 0.6 is 0.4790946683486034\n",
      "Our f1 score for threshold 0.7 is 0.23730646809554073\n",
      "Our f1 score for threshold 0.8 is 0.16950999423858204\n",
      "Our f1 score for threshold 0.9 is 0.16946043458977578\n",
      "Our best score is 0.7731480665958421 and has a threshold 0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7731480665958421, 0.2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_thresholds(train, np.hstack(dba_embs), 0, 50, 'cosine', 0, 1, 0.1, verbose=True)\n",
    "search_thresholds(train, np.mean(dba_embs, axis=0), 0, 50, 'cosine', 0, 1, 0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "comfortable-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our f1 score for threshold 0.0 is 0.212138935893935\n",
      "Our f1 score for threshold 0.1 is 0.732263754640866\n",
      "Our f1 score for threshold 0.2 is 0.7731480665958421\n",
      "Our f1 score for threshold 0.3 is 0.7729552706400833\n",
      "Our f1 score for threshold 0.4 is 0.7407169284205879\n",
      "Our f1 score for threshold 0.5 is 0.6566666768203135\n",
      "Our f1 score for threshold 0.6 is 0.4790946683486034\n",
      "Our f1 score for threshold 0.7 is 0.23730646809554073\n",
      "Our f1 score for threshold 0.8 is 0.16950999423858204\n",
      "Our f1 score for threshold 0.9 is 0.16946043458977578\n",
      "Our best score is 0.7731480665958421 and has a threshold 0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7731480665958421, 0.2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dba_emb_ens = np.mean(dba_embs, axis=0)\n",
    "search_thresholds(train, dba_emb_ens, 0, 50, 'cosine', 0, 1, 0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "phantom-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embs\n",
    "bert_emb = pickle.load(open(base_dir / '../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/embeddings_epoch15.pkl', 'rb'))\n",
    "embs.append(bert_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "approved-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.8633810383660473 and has a threshold 0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8633810383660473, 0.3)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_thresholds(train, embs[-1], 0, 50, 'cosine', 0.2, 0.5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "knowing-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4c60d6bf0f471f86aa3f6fd1d343db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.8413435221569675 and has a threshold 0.4\n",
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae72557bedd4a9088622afe65529b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.8687072799195111 and has a threshold 0.4\n",
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073bb759eb9c4eb2978828acfb1e5ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.8729258070653391 and has a threshold 0.30000000000000004\n",
      "5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a57daac73a74969b5543968e225c74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our best score is 0.8689642330462329 and has a threshold 0.2\n"
     ]
    }
   ],
   "source": [
    "embs_ens = np.hstack(embs)\n",
    "for n_aug in range(2, 6):\n",
    "    print(n_aug)\n",
    "    dba_embs_ens = database_augment(embs_ens, n_aug=n_aug)\n",
    "    search_thresholds(train, dba_embs_ens, 0, 50, 'cosine', 0, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-poultry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
