{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from shutil import copyfile\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electronic-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    outdir: str = \"../results/bert\"\n",
    "    device: str = \"cuda:2\"\n",
    "    device_id: int = 2\n",
    "\n",
    "    datadir: str = '../data/tfrecord-skf'\n",
    "    modeldir: str = '../models/bert/bert_en_uncased_L-24_H-1024_A-16_1'\n",
    "    seed: int = 123\n",
    "    n_splits: int = 3\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 25\n",
    "    patience: int = 5\n",
    "    lr: float = 0.00001\n",
    "    encode_len: int = 70\n",
    "    emb_len: int = 2048\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intermediate-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path().resolve()\n",
    "sys.path.append(os.path.abspath(base_dir / '../'))\n",
    "\n",
    "config_dict = {\n",
    "#     'epochs': 1,\n",
    "}\n",
    "\n",
    "config = Config().update(config_dict)\n",
    "config.to_yaml(base_dir / config.outdir / 'config.yaml')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.device_id)\n",
    "\n",
    "\n",
    "from src.tokenization import *\n",
    "from src.preprocess import *\n",
    "from src.text import *\n",
    "from src.model import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supported-investor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>f1</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>0</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>1</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>3</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>4</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret            0   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...            1   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr            2   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...            3   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml            4   \n",
       "\n",
       "                             matches        f1  fold  \n",
       "0   train_129225211 train_2278313361  0.666667     1  \n",
       "1  train_3386243561 train_3423213080  0.666667     2  \n",
       "2  train_2288590299 train_3803689425  0.666667     5  \n",
       "3  train_2406599165 train_3342059966  0.666667     7  \n",
       "4   train_3369186413 train_921438619  0.666667     9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv(base_dir / config.datadir / 'train.csv')\n",
    "# train = prepare_dataset(df=train, n_splits=config.n_splits, seed=config.seed)\n",
    "\n",
    "train = pd.read_csv(base_dir / config.datadir / 'train_folds.csv')\n",
    "n_classes = train['label_group'].nunique()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "uniform-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_folds(fold: int):\n",
    "    if fold == -1:\n",
    "        train_folds = list(range(train.fold.unique().shape[0]))\n",
    "        valid_folds = train_folds[:3]\n",
    "        return train_folds, valid_folds\n",
    "    \n",
    "    train_folds, valid_folds = list(), list()\n",
    "    for i in range(train.fold.unique().shape[0]):\n",
    "        if i % config.n_splits == fold:\n",
    "            valid_folds += [i]\n",
    "        else:\n",
    "            train_folds += [i]\n",
    "            \n",
    "    return train_folds, valid_folds\n",
    "\n",
    "\n",
    "def get_lr_callback():\n",
    "    lr_start   = 0.00001\n",
    "    lr_max     = 0.0001\n",
    "    lr_min     = 0.00001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "german-billion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 1/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 23.5025 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 00001: val_loss improved from inf to 22.42700, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch01.h5\n",
      "1071/1071 [==============================] - 560s 523ms/step - loss: 23.5025 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.4270 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 2.8000000000000003e-05.\n",
      "Epoch 2/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 21.7377 - sparse_categorical_accuracy: 0.0051\n",
      "Epoch 00002: val_loss improved from 22.42700 to 18.88433, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch02.h5\n",
      "1071/1071 [==============================] - 554s 517ms/step - loss: 21.7377 - sparse_categorical_accuracy: 0.0051 - val_loss: 18.8843 - val_sparse_categorical_accuracy: 0.0250\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 4.6e-05.\n",
      "Epoch 3/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 18.6172 - sparse_categorical_accuracy: 0.0302\n",
      "Epoch 00003: val_loss improved from 18.88433 to 14.95674, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch03.h5\n",
      "1071/1071 [==============================] - 554s 517ms/step - loss: 18.6172 - sparse_categorical_accuracy: 0.0302 - val_loss: 14.9567 - val_sparse_categorical_accuracy: 0.0714\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 6.4e-05.\n",
      "Epoch 4/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 15.3961 - sparse_categorical_accuracy: 0.0745\n",
      "Epoch 00004: val_loss improved from 14.95674 to 11.71636, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch04.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 15.3961 - sparse_categorical_accuracy: 0.0745 - val_loss: 11.7164 - val_sparse_categorical_accuracy: 0.1429\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 8.2e-05.\n",
      "Epoch 5/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 12.6136 - sparse_categorical_accuracy: 0.1304\n",
      "Epoch 00005: val_loss improved from 11.71636 to 9.05651, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch05.h5\n",
      "1071/1071 [==============================] - 554s 517ms/step - loss: 12.6136 - sparse_categorical_accuracy: 0.1304 - val_loss: 9.0565 - val_sparse_categorical_accuracy: 0.2250\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 10.2653 - sparse_categorical_accuracy: 0.2007\n",
      "Epoch 00006: val_loss improved from 9.05651 to 6.91113, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch06.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 10.2653 - sparse_categorical_accuracy: 0.2007 - val_loss: 6.9111 - val_sparse_categorical_accuracy: 0.3164\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 8.2e-05.\n",
      "Epoch 7/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 7.2612 - sparse_categorical_accuracy: 0.3227\n",
      "Epoch 00007: val_loss improved from 6.91113 to 4.01706, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch07.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 7.2612 - sparse_categorical_accuracy: 0.3227 - val_loss: 4.0171 - val_sparse_categorical_accuracy: 0.5391\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 6.760000000000002e-05.\n",
      "Epoch 8/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 4.7193 - sparse_categorical_accuracy: 0.4888\n",
      "Epoch 00008: val_loss improved from 4.01706 to 2.15777, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch08.h5\n",
      "1071/1071 [==============================] - 553s 516ms/step - loss: 4.7193 - sparse_categorical_accuracy: 0.4888 - val_loss: 2.1578 - val_sparse_categorical_accuracy: 0.7341\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 5.608000000000001e-05.\n",
      "Epoch 9/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 2.9306 - sparse_categorical_accuracy: 0.6694\n",
      "Epoch 00009: val_loss improved from 2.15777 to 1.12487, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch09.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 2.9306 - sparse_categorical_accuracy: 0.6694 - val_loss: 1.1249 - val_sparse_categorical_accuracy: 0.8571\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 4.686400000000001e-05.\n",
      "Epoch 10/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 1.7725 - sparse_categorical_accuracy: 0.8011\n",
      "Epoch 00010: val_loss improved from 1.12487 to 0.59701, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch10.h5\n",
      "1071/1071 [==============================] - 553s 516ms/step - loss: 1.7725 - sparse_categorical_accuracy: 0.8011 - val_loss: 0.5970 - val_sparse_categorical_accuracy: 0.9305\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 3.949120000000001e-05.\n",
      "Epoch 11/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 1.1117 - sparse_categorical_accuracy: 0.8880\n",
      "Epoch 00011: val_loss improved from 0.59701 to 0.34818, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch11.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 1.1117 - sparse_categorical_accuracy: 0.8880 - val_loss: 0.3482 - val_sparse_categorical_accuracy: 0.9607\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 3.359296000000001e-05.\n",
      "Epoch 12/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.7386 - sparse_categorical_accuracy: 0.9369\n",
      "Epoch 00012: val_loss improved from 0.34818 to 0.24561, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch12.h5\n",
      "1071/1071 [==============================] - 553s 516ms/step - loss: 0.7386 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.2456 - val_sparse_categorical_accuracy: 0.9748\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 2.887436800000001e-05.\n",
      "Epoch 13/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.5120 - sparse_categorical_accuracy: 0.9620\n",
      "Epoch 00013: val_loss improved from 0.24561 to 0.19477, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch13.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.5120 - sparse_categorical_accuracy: 0.9620 - val_loss: 0.1948 - val_sparse_categorical_accuracy: 0.9825\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2.509949440000001e-05.\n",
      "Epoch 14/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.3776 - sparse_categorical_accuracy: 0.9738\n",
      "Epoch 00014: val_loss improved from 0.19477 to 0.15020, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch14.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.3776 - sparse_categorical_accuracy: 0.9738 - val_loss: 0.1502 - val_sparse_categorical_accuracy: 0.9872\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2.207959552000001e-05.\n",
      "Epoch 15/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.3065 - sparse_categorical_accuracy: 0.9805\n",
      "Epoch 00015: val_loss improved from 0.15020 to 0.14205, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch15.h5\n",
      "1071/1071 [==============================] - 553s 516ms/step - loss: 0.3065 - sparse_categorical_accuracy: 0.9805 - val_loss: 0.1420 - val_sparse_categorical_accuracy: 0.9896\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.9663676416000005e-05.\n",
      "Epoch 16/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.2542 - sparse_categorical_accuracy: 0.9847\n",
      "Epoch 00016: val_loss improved from 0.14205 to 0.11836, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch16.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.2542 - sparse_categorical_accuracy: 0.9847 - val_loss: 0.1184 - val_sparse_categorical_accuracy: 0.9914\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.7730941132800006e-05.\n",
      "Epoch 17/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.2195 - sparse_categorical_accuracy: 0.9872\n",
      "Epoch 00017: val_loss improved from 0.11836 to 0.10043, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch17.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.2195 - sparse_categorical_accuracy: 0.9872 - val_loss: 0.1004 - val_sparse_categorical_accuracy: 0.9937\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.6184752906240005e-05.\n",
      "Epoch 18/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1891 - sparse_categorical_accuracy: 0.9896\n",
      "Epoch 00018: val_loss improved from 0.10043 to 0.09857, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch18.h5\n",
      "1071/1071 [==============================] - 554s 517ms/step - loss: 0.1891 - sparse_categorical_accuracy: 0.9896 - val_loss: 0.0986 - val_sparse_categorical_accuracy: 0.9937\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.4947802324992005e-05.\n",
      "Epoch 19/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1717 - sparse_categorical_accuracy: 0.9899\n",
      "Epoch 00019: val_loss improved from 0.09857 to 0.09318, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch19.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.1717 - sparse_categorical_accuracy: 0.9899 - val_loss: 0.0932 - val_sparse_categorical_accuracy: 0.9946\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.3958241859993605e-05.\n",
      "Epoch 20/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1621 - sparse_categorical_accuracy: 0.9908\n",
      "Epoch 00020: val_loss did not improve from 0.09318\n",
      "1071/1071 [==============================] - 552s 515ms/step - loss: 0.1621 - sparse_categorical_accuracy: 0.9908 - val_loss: 0.0964 - val_sparse_categorical_accuracy: 0.9945\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.3166593487994884e-05.\n",
      "Epoch 21/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1577 - sparse_categorical_accuracy: 0.9907\n",
      "Epoch 00021: val_loss did not improve from 0.09318\n",
      "1071/1071 [==============================] - 551s 515ms/step - loss: 0.1577 - sparse_categorical_accuracy: 0.9907 - val_loss: 0.1029 - val_sparse_categorical_accuracy: 0.9945\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.2533274790395908e-05.\n",
      "Epoch 22/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1484 - sparse_categorical_accuracy: 0.9915\n",
      "Epoch 00022: val_loss improved from 0.09318 to 0.08755, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch22.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.1484 - sparse_categorical_accuracy: 0.9915 - val_loss: 0.0876 - val_sparse_categorical_accuracy: 0.9949\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.2026619832316725e-05.\n",
      "Epoch 23/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1424 - sparse_categorical_accuracy: 0.9917\n",
      "Epoch 00023: val_loss improved from 0.08755 to 0.08683, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch23.h5\n",
      "1071/1071 [==============================] - 553s 517ms/step - loss: 0.1424 - sparse_categorical_accuracy: 0.9917 - val_loss: 0.0868 - val_sparse_categorical_accuracy: 0.9952\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.1621295865853382e-05.\n",
      "Epoch 24/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1364 - sparse_categorical_accuracy: 0.9919\n",
      "Epoch 00024: val_loss improved from 0.08683 to 0.08290, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch24.h5\n",
      "1071/1071 [==============================] - 554s 517ms/step - loss: 0.1364 - sparse_categorical_accuracy: 0.9919 - val_loss: 0.0829 - val_sparse_categorical_accuracy: 0.9955\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.1297036692682704e-05.\n",
      "Epoch 25/25\n",
      "1071/1071 [==============================] - ETA: 0s - loss: 0.1346 - sparse_categorical_accuracy: 0.9925\n",
      "Epoch 00025: val_loss improved from 0.08290 to 0.08243, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert/Bert_seed123_encodelen70_emb2048_fold--1/epoch25.h5\n",
      "1071/1071 [==============================] - 556s 519ms/step - loss: 0.1346 - sparse_categorical_accuracy: 0.9925 - val_loss: 0.0824 - val_sparse_categorical_accuracy: 0.9953\n"
     ]
    }
   ],
   "source": [
    "fold = -1\n",
    "seed_everything(config.seed)\n",
    "\n",
    "outdir = base_dir / config.outdir / f'Bert_seed{config.seed}_encodelen{config.encode_len}_emb{config.emb_len}_fold-{fold}'\n",
    "os.makedirs(str(outdir), exist_ok=True)\n",
    "\n",
    "train_folds, valid_folds = split_folds(fold=fold)\n",
    "train_df, valid_df = train.query('fold in @train_folds'), train.query('fold in @valid_folds')\n",
    "\n",
    "bert_layer = hub.KerasLayer(str(base_dir / config.modeldir), trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "x_train = bert_encode(train_df['title'].values, tokenizer, max_len=config.encode_len)\n",
    "x_val = bert_encode(valid_df['title'].values, tokenizer, max_len=config.encode_len)\n",
    "y_train = train_df['label_group'].values\n",
    "y_val = valid_df['label_group'].values\n",
    "\n",
    "x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
    "x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
    "\n",
    "bert_model = build_bert_model(bert_layer, n_classes=n_classes, lr=config.lr, max_len=config.encode_len, emb_len=config.emb_len)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    str(outdir / 'epoch{epoch:02d}.h5'),\n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    save_weights_only = True, \n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "\n",
    "history = bert_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data = (x_val, y_val),\n",
    "    epochs = config.epochs, \n",
    "    callbacks = [checkpoint, get_lr_callback()],\n",
    "    batch_size = config.batch_size,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "pickle.dump(history.history, open(str(outdir / 'history.pkl'), 'wb'))\n",
    "\n",
    "del bert_model, bert_layer, train_df, valid_df, x_train, x_val, y_train, y_val\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dressed-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fold in range(config.n_splits):\n",
    "#     seed_everything(config.seed)\n",
    "\n",
    "#     outdir = base_dir / config.outdir / f'Bert_seed{config.seed}_encodelen{config.encode_len}_fold-{fold}'\n",
    "#     os.makedirs(str(outdir), exist_ok=True)\n",
    "    \n",
    "#     train_folds, valid_folds = split_folds(fold=fold)\n",
    "#     train_df, valid_df = train.query('fold in @train_folds'), train.query('fold in @valid_folds')\n",
    "    \n",
    "#     bert_layer = hub.KerasLayer(str(base_dir / config.modeldir), trainable=True)\n",
    "#     vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "#     do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "#     tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "    \n",
    "#     x_train = bert_encode(train_df['title'].values, tokenizer, max_len=config.encode_len)\n",
    "#     x_val = bert_encode(valid_df['title'].values, tokenizer, max_len=config.encode_len)\n",
    "#     y_train = train_df['label_group'].values\n",
    "#     y_val = valid_df['label_group'].values\n",
    "    \n",
    "#     x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
    "#     x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
    "    \n",
    "#     bert_model = build_bert_model(bert_layer, n_classes=n_classes, lr=config.lr, max_len=config.encode_len)\n",
    "    \n",
    "#     checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "#         str(outdir / 'epoch{epoch:02d}.h5'),\n",
    "#         monitor = 'val_loss', \n",
    "#         verbose = 1, \n",
    "#         save_best_only = True,\n",
    "#         save_weights_only = True, \n",
    "#         mode = 'min'\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     history = bert_model.fit(\n",
    "#         x_train, y_train,\n",
    "#         validation_data = (x_val, y_val),\n",
    "#         epochs = config.epochs, \n",
    "#         callbacks = [checkpoint],\n",
    "#         batch_size = config.batch_size,\n",
    "#         verbose = 1\n",
    "#     )\n",
    "    \n",
    "#     pickle.dump(history.history, open(str(outdir / 'history.pkl'), 'wb'))\n",
    "    \n",
    "#     del bert_model, bert_layer, train_df, valid_df, x_train, x_val, y_train, y_val\n",
    "#     gc.collect()\n",
    "#     tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-medicine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
