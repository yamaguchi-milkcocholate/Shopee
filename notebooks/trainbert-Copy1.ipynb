{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "disciplinary-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "from typing import *\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from shutil import copyfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "muslim-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    outdir: str = \"../results/bert-gkf\"\n",
    "    device: str = \"cuda:1\"\n",
    "    device_id: int = 1\n",
    "\n",
    "    datadir: str = '../data/tfrecord-skf'\n",
    "    modeldir: str = '../models/bert/bert_en_uncased_L-24_H-1024_A-16_1'\n",
    "    seed: int = 123\n",
    "    n_splits: int = 3\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 25\n",
    "    patience: int = 5\n",
    "    lr: float = 0.00001\n",
    "    encode_len: int = 70\n",
    "    emb_len: int = 2048\n",
    "\n",
    "    def update(self, param_dict: Dict) -> \"Config\":\n",
    "        # Overwrite by `param_dict`\n",
    "        for key, value in param_dict.items():\n",
    "            if not hasattr(self, key):\n",
    "                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n",
    "            setattr(self, key, value)\n",
    "        return self\n",
    "    \n",
    "    def to_yaml(self, filepath: str, width: int = 120):\n",
    "        with open(filepath, 'w') as f:\n",
    "            yaml.dump(asdict(self), f, width=width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "romantic-button",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weighted-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path().resolve()\n",
    "sys.path.append(os.path.abspath(base_dir / '../'))\n",
    "\n",
    "config_dict = {\n",
    "#     'epochs': 1,\n",
    "}\n",
    "\n",
    "config = Config().update(config_dict)\n",
    "config.to_yaml(base_dir / config.outdir / 'config.yaml')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.device_id)\n",
    "\n",
    "\n",
    "from src.tokenization import *\n",
    "from src.preprocess import *\n",
    "from src.text import *\n",
    "from src.model import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hidden-associate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "      <th>f1</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>0</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>1</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_2464356923</td>\n",
       "      <td>0013e7355ffc5ff8fb1ccad3e42d92fe.jpg</td>\n",
       "      <td>bbd097a7870f4a50</td>\n",
       "      <td>CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...</td>\n",
       "      <td>2</td>\n",
       "      <td>train_2464356923 train_2753295474 train_305884580</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_86570404</td>\n",
       "      <td>0019a3c6755a194cb2e2c12bfc63972e.jpg</td>\n",
       "      <td>ea9af4f483249972</td>\n",
       "      <td>[LOGU] Tempelan kulkas magnet angka, tempelan ...</td>\n",
       "      <td>3</td>\n",
       "      <td>train_86570404 train_2837452969 train_77364776</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_831680791</td>\n",
       "      <td>001be52b2beec40ddc1d2d7fc7a68f08.jpg</td>\n",
       "      <td>e1ce953d1a70618f</td>\n",
       "      <td>BIG SALE SEPATU PANTOFEL KULIT KEREN KERJA KAN...</td>\n",
       "      <td>4</td>\n",
       "      <td>train_831680791 train_3031035861</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "5  train_2464356923  0013e7355ffc5ff8fb1ccad3e42d92fe.jpg  bbd097a7870f4a50   \n",
       "8    train_86570404  0019a3c6755a194cb2e2c12bfc63972e.jpg  ea9af4f483249972   \n",
       "9   train_831680791  001be52b2beec40ddc1d2d7fc7a68f08.jpg  e1ce953d1a70618f   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret            0   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...            1   \n",
       "5  CELANA WANITA  (BB 45-84 KG)Harem wanita (bisa...            2   \n",
       "8  [LOGU] Tempelan kulkas magnet angka, tempelan ...            3   \n",
       "9  BIG SALE SEPATU PANTOFEL KULIT KEREN KERJA KAN...            4   \n",
       "\n",
       "                                             matches        f1  fold  \n",
       "0                   train_129225211 train_2278313361  0.666667     0  \n",
       "1                  train_3386243561 train_3423213080  0.666667     0  \n",
       "5  train_2464356923 train_2753295474 train_305884580  0.500000     0  \n",
       "8     train_86570404 train_2837452969 train_77364776  0.500000     0  \n",
       "9                   train_831680791 train_3031035861  0.666667     0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv(base_dir / config.datadir / 'train.csv')\n",
    "# train = prepare_dataset(df=train, n_splits=config.n_splits, seed=config.seed)\n",
    "\n",
    "train = pd.read_csv(base_dir / config.datadir / 'train_folds.csv')\n",
    "train_labels, test_labels = train_test_split(train.label_group.unique(), test_size=0.3, random_state=config.seed)\n",
    "train.loc[train.label_group.isin(train_labels), 'fold'] = 0\n",
    "train.loc[train.label_group.isin(test_labels), 'fold'] = 1\n",
    "train.to_csv(base_dir / config.outdir / 'train_fold.csv', index=False)\n",
    "\n",
    "train = train.query(f'fold == 0')\n",
    "labelconvmap = {label: i for i, label in enumerate(sorted(train.label_group.unique()))}\n",
    "train['label_group'] = train.label_group.map(labelconvmap)\n",
    "\n",
    "n_classes = train['label_group'].nunique()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foreign-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_callback():\n",
    "    lr_start   = 0.00001\n",
    "    lr_max     = 0.0001\n",
    "    lr_min     = 0.00001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max    \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "necessary-village",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "Epoch 1/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 23.2021 - sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 00001: loss improved from inf to 23.20206, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch01.h5\n",
      "748/748 [==============================] - 365s 488ms/step - loss: 23.2021 - sparse_categorical_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 2.8000000000000003e-05.\n",
      "Epoch 2/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 21.5235 - sparse_categorical_accuracy: 0.0060\n",
      "Epoch 00002: loss improved from 23.20206 to 21.52355, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch02.h5\n",
      "748/748 [==============================] - 365s 488ms/step - loss: 21.5235 - sparse_categorical_accuracy: 0.0060\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 4.6e-05.\n",
      "Epoch 3/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 18.5843 - sparse_categorical_accuracy: 0.0315\n",
      "Epoch 00003: loss improved from 21.52355 to 18.58431, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch03.h5\n",
      "748/748 [==============================] - 365s 488ms/step - loss: 18.5843 - sparse_categorical_accuracy: 0.0315\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 6.4e-05.\n",
      "Epoch 4/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 15.5044 - sparse_categorical_accuracy: 0.0752\n",
      "Epoch 00004: loss improved from 18.58431 to 15.50438, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch04.h5\n",
      "748/748 [==============================] - 365s 487ms/step - loss: 15.5044 - sparse_categorical_accuracy: 0.0752\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 8.2e-05.\n",
      "Epoch 5/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 12.6843 - sparse_categorical_accuracy: 0.1354\n",
      "Epoch 00005: loss improved from 15.50438 to 12.68429, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch05.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 12.6843 - sparse_categorical_accuracy: 0.1354\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 6/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 10.3256 - sparse_categorical_accuracy: 0.1993\n",
      "Epoch 00006: loss improved from 12.68429 to 10.32555, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch06.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 10.3256 - sparse_categorical_accuracy: 0.1993\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 8.2e-05.\n",
      "Epoch 7/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 7.3766 - sparse_categorical_accuracy: 0.3180\n",
      "Epoch 00007: loss improved from 10.32555 to 7.37658, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch07.h5\n",
      "748/748 [==============================] - 362s 485ms/step - loss: 7.3766 - sparse_categorical_accuracy: 0.3180\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 6.760000000000002e-05.\n",
      "Epoch 8/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 4.8000 - sparse_categorical_accuracy: 0.4708\n",
      "Epoch 00008: loss improved from 7.37658 to 4.80002, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch08.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 4.8000 - sparse_categorical_accuracy: 0.4708\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 5.608000000000001e-05.\n",
      "Epoch 9/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 3.0128 - sparse_categorical_accuracy: 0.6509\n",
      "Epoch 00009: loss improved from 4.80002 to 3.01281, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch09.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 3.0128 - sparse_categorical_accuracy: 0.6509\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 4.686400000000001e-05.\n",
      "Epoch 10/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 1.8784 - sparse_categorical_accuracy: 0.7925\n",
      "Epoch 00010: loss improved from 3.01281 to 1.87840, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch10.h5\n",
      "748/748 [==============================] - 362s 485ms/step - loss: 1.8784 - sparse_categorical_accuracy: 0.7925\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 3.949120000000001e-05.\n",
      "Epoch 11/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 1.1494 - sparse_categorical_accuracy: 0.8885\n",
      "Epoch 00011: loss improved from 1.87840 to 1.14939, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch11.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 1.1494 - sparse_categorical_accuracy: 0.8885\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 3.359296000000001e-05.\n",
      "Epoch 12/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.7228 - sparse_categorical_accuracy: 0.9442\n",
      "Epoch 00012: loss improved from 1.14939 to 0.72284, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch12.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.7228 - sparse_categorical_accuracy: 0.9442\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 2.887436800000001e-05.\n",
      "Epoch 13/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.4870 - sparse_categorical_accuracy: 0.9693\n",
      "Epoch 00013: loss improved from 0.72284 to 0.48699, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch13.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.4870 - sparse_categorical_accuracy: 0.9693\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 2.509949440000001e-05.\n",
      "Epoch 14/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.3602 - sparse_categorical_accuracy: 0.9799\n",
      "Epoch 00014: loss improved from 0.48699 to 0.36021, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch14.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.3602 - sparse_categorical_accuracy: 0.9799\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 2.207959552000001e-05.\n",
      "Epoch 15/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.2764 - sparse_categorical_accuracy: 0.9857\n",
      "Epoch 00015: loss improved from 0.36021 to 0.27644, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch15.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.2764 - sparse_categorical_accuracy: 0.9857\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 1.9663676416000005e-05.\n",
      "Epoch 16/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.2285 - sparse_categorical_accuracy: 0.9888\n",
      "Epoch 00016: loss improved from 0.27644 to 0.22849, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch16.h5\n",
      "748/748 [==============================] - 362s 485ms/step - loss: 0.2285 - sparse_categorical_accuracy: 0.9888\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 1.7730941132800006e-05.\n",
      "Epoch 17/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.2056 - sparse_categorical_accuracy: 0.9895\n",
      "Epoch 00017: loss improved from 0.22849 to 0.20559, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch17.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.2056 - sparse_categorical_accuracy: 0.9895\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 1.6184752906240005e-05.\n",
      "Epoch 18/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1699 - sparse_categorical_accuracy: 0.9918\n",
      "Epoch 00018: loss improved from 0.20559 to 0.16994, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch18.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.1699 - sparse_categorical_accuracy: 0.9918\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 1.4947802324992005e-05.\n",
      "Epoch 19/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1539 - sparse_categorical_accuracy: 0.9924\n",
      "Epoch 00019: loss improved from 0.16994 to 0.15387, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch19.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.1539 - sparse_categorical_accuracy: 0.9924\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 1.3958241859993605e-05.\n",
      "Epoch 20/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1409 - sparse_categorical_accuracy: 0.9929\n",
      "Epoch 00020: loss improved from 0.15387 to 0.14087, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch20.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.1409 - sparse_categorical_accuracy: 0.9929\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 1.3166593487994884e-05.\n",
      "Epoch 21/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1293 - sparse_categorical_accuracy: 0.9934\n",
      "Epoch 00021: loss improved from 0.14087 to 0.12932, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch21.h5\n",
      "748/748 [==============================] - 362s 484ms/step - loss: 0.1293 - sparse_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 1.2533274790395908e-05.\n",
      "Epoch 22/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1239 - sparse_categorical_accuracy: 0.9934\n",
      "Epoch 00022: loss improved from 0.12932 to 0.12389, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch22.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.1239 - sparse_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 1.2026619832316725e-05.\n",
      "Epoch 23/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1154 - sparse_categorical_accuracy: 0.9942\n",
      "Epoch 00023: loss improved from 0.12389 to 0.11537, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch23.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.1154 - sparse_categorical_accuracy: 0.9942\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 1.1621295865853382e-05.\n",
      "Epoch 24/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1082 - sparse_categorical_accuracy: 0.9945\n",
      "Epoch 00024: loss improved from 0.11537 to 0.10818, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch24.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.1082 - sparse_categorical_accuracy: 0.9945\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 1.1297036692682704e-05.\n",
      "Epoch 25/25\n",
      "748/748 [==============================] - ETA: 0s - loss: 0.1025 - sparse_categorical_accuracy: 0.9946\n",
      "Epoch 00025: loss improved from 0.10818 to 0.10251, saving model to /home/yamaguchi-milkcocholate/Shopee/notebooks/../results/bert-gkf/Bert_seed123_encodelen70_emb2048-gkf/epoch25.h5\n",
      "748/748 [==============================] - 363s 485ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "seed_everything(config.seed)\n",
    "\n",
    "outdir = base_dir / config.outdir / f'Bert_seed{config.seed}_encodelen{config.encode_len}_emb{config.emb_len}-gkf'\n",
    "os.makedirs(str(outdir), exist_ok=True)\n",
    "\n",
    "bert_layer = hub.KerasLayer(str(base_dir / config.modeldir), trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "x_train = bert_encode(train['title'].values, tokenizer, max_len=config.encode_len)\n",
    "y_train = train['label_group'].values\n",
    "\n",
    "x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
    "\n",
    "bert_model = build_bert_model(bert_layer, n_classes=n_classes, lr=config.lr, max_len=config.encode_len, emb_len=config.emb_len)\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    str(outdir / 'epoch{epoch:02d}.h5'),\n",
    "    monitor = 'loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True,\n",
    "    save_weights_only = True, \n",
    "    mode = 'min'\n",
    ")\n",
    "\n",
    "\n",
    "history = bert_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs = config.epochs, \n",
    "    callbacks = [checkpoint, get_lr_callback()],\n",
    "    batch_size = config.batch_size,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "pickle.dump(history.history, open(str(outdir / 'history.pkl'), 'wb'))\n",
    "\n",
    "del bert_model, bert_layer, train, x_train, y_train\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-electron",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-vermont",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
